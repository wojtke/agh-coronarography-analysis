{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe0c521-c218-4998-8f0c-e62c9c7b32a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/przemek/miniconda3/envs/med/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f3a14669810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload all\n",
    "    \n",
    "from utils import read_images\n",
    "from datasets import DominanceDataset\n",
    "from vit import SiameseViT\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e64b6e-fb6f-4a57-9015-060567565599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test images from cache.\n"
     ]
    }
   ],
   "source": [
    "model = SiameseViT()\n",
    "model.load_state_dict(torch.load('weights/dominance.pth'))\n",
    "model.eval()\n",
    "\n",
    "images = read_images(ids=\"test\")\n",
    "test_dataset = DominanceDataset(images, split=\"test\")\n",
    "#test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "176ec457-3ee3-4d90-9922-9db4022b00b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(left_imgs, right_imgs):\n",
    "    preds = []\n",
    "    for l in left_imgs:\n",
    "        for r in right_imgs:\n",
    "            pred = torch.sigmoid(model(l, r))[0, 1]\n",
    "            preds.append(pred)\n",
    "    preds = np.array(preds)\n",
    "    return preds.mean(), preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f0bea12-0fe3-4587-9751-1f512bebd595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval representative frame sampling \n",
    "\n",
    "labels = []\n",
    "partial_labels = []\n",
    "final_preds = []\n",
    "partial_preds = []\n",
    "for i in range(len(test_dataset)):\n",
    "    study_id, data = test_dataset.index[i]\n",
    "\n",
    "    imgs_left=[]\n",
    "    for img_id, frame_n in zip(data[\"l\"], data[\"l_f\"]):\n",
    "        img = images[img_id][frame_n]\n",
    "        img = test_dataset.transform(img).unsqueeze(0)\n",
    "        imgs_left.append(img)\n",
    "\n",
    "    imgs_right=[]\n",
    "    for img_id, frame_n in zip(data[\"r\"], data[\"r_f\"]):\n",
    "        img = images[img_id][frame_n]\n",
    "        img = test_dataset.transform(img).unsqueeze(0)\n",
    "        imgs_right.append(img)\n",
    "\n",
    "    pred, all_preds = get_pred(imgs_left, imgs_right)\n",
    "    final_preds.append(1 if pred > 0.5 else 0)\n",
    "    partial_preds.extend([1 if p > 0.5 else 0 for p in all_preds])\n",
    "    labels.append(data[\"label\"])\n",
    "    partial_labels.extend([data[\"label\"]]*len(all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88694cbb-16c0-4f43-997f-8fef5885fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.67      0.57         3\n",
      "         1.0       0.93      0.88      0.90        16\n",
      "\n",
      "    accuracy                           0.84        19\n",
      "   macro avg       0.72      0.77      0.74        19\n",
      "weighted avg       0.86      0.84      0.85        19\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2  1]\n",
      " [ 2 14]]\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(labels, final_preds)\n",
    "matrix = confusion_matrix(labels, final_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0710593-8e2c-4111-8daf-06e9e4edf314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.27      0.89      0.42        28\n",
      "         1.0       0.99      0.76      0.86       280\n",
      "\n",
      "    accuracy                           0.78       308\n",
      "   macro avg       0.63      0.83      0.64       308\n",
      "weighted avg       0.92      0.78      0.82       308\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 25   3]\n",
      " [ 66 214]]\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(partial_labels, partial_preds)\n",
    "matrix = confusion_matrix(partial_labels, partial_preds)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462e2d6-2633-49f1-9393-b20735cdf488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "med",
   "language": "python",
   "name": "med"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
